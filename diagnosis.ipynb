{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigating Spey fitting of a histogram from Contur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import numpy as np\n",
    "import spey\n",
    "spey.set_log_level(3) # debug\n",
    "\n",
    "# load some data from a contur histogram\n",
    "bg = np.array([3.612882e-03, 1.598201e-03, 3.186370e-05])\n",
    "meas = np.array([3.2194e-03, 1.4437e-03, 3.0542e-05])\n",
    "signal = np.array([1.26975467e-06, 0.00000000e+00, 0.00000000e+00])\n",
    "measurement_cov = np.array([[1.25297487e-07, 3.41845046e-08, 1.09669171e-09],[3.41845046e-08, 1.19312634e-08, 3.20895254e-10],[1.09669171e-09, 3.20895254e-10, 1.88543549e-11]])\n",
    "background_cov = np.array([[4.37507776e-08, 0.00000000e+00, 0.00000000e+00], [0.00000000e+00, 4.15880810e-09, 0.00000000e+00],[0.00000000e+00, 0.00000000e+00, 5.55546207e-13]])\n",
    "signal_cov = np.array([[1.72952086e-12, 0.00000000e+00, 0.00000000e+00],[0.00000000e+00, 8.52595599e-15, 0.00000000e+00],[0.00000000e+00, 0.00000000e+00, 6.63785501e-17]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate the calculation currently implemented in Contur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "inv_cov_s_b = np.linalg.inv(signal_cov+measurement_cov+background_cov)\n",
    "inv_cov_b_only = np.linalg.inv(measurement_cov+background_cov)\n",
    "\n",
    "# contstruct a chi_square test statistic in the asymptotic limit\n",
    "delta_mu_test = signal + bg - meas\n",
    "delta_mu_null = bg - meas\n",
    "ts_s_b = np.dot(delta_mu_test,np.dot(inv_cov_s_b,delta_mu_test))\n",
    "ts_b = np.dot(delta_mu_null,np.dot(inv_cov_s_b,delta_mu_null))\n",
    "\n",
    "# the test statistics are chi-sq distributed, so calculate the log(p-value)\n",
    "log_pval_sb, log_pval_b = norm.logsf(np.sqrt((ts_s_b,ts_b)))\n",
    "\n",
    "# convert to CLs value\n",
    "CLs = 1 - np.exp(log_pval_sb - log_pval_b)\n",
    "print(f'Current Contur CLs: {CLs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spey CLs Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the contur plugin\n",
    "contur_model = spey.get_backend(\"contur.full_histogram_likelihood\")(\n",
    "    signal_yields=signal,\n",
    "    background_yields=bg,\n",
    "    data=meas,\n",
    "    signal_covariance=signal_cov,\n",
    "    background_covariance=background_cov,\n",
    "    data_covariance=measurement_cov\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build some default spey models for comparison\n",
    "corr_bkd = spey.get_backend('default_pdf.correlated_background')(\n",
    "    signal_yields=signal,\n",
    "    background_yields=bg,\n",
    "    data=meas,\n",
    "    covariance_matrix=background_cov,\n",
    ")\n",
    "\n",
    "uncorr_bkd = spey.get_backend('default_pdf.uncorrelated_background')(signal_yields=signal,\n",
    "                              background_yields=bg,\n",
    "                              data=meas,\n",
    "                              absolute_uncertainties=np.sqrt(np.diag(background_cov))\n",
    ")\n",
    "\n",
    "multinorm = spey.get_backend('default_pdf.multivariate_normal')(signal_yields=signal,\n",
    "                              background_yields=bg,\n",
    "                              data=meas,\n",
    "                              covariance_matrix=signal_cov+background_cov+measurement_cov\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLs computation for each model\n",
    "\n",
    "All models give a CLs of 1.0 when the asymptotic calculator is used\n",
    "\n",
    "The three models involving Poisson terms give a very low CLs value when the chi-squared calculator is used\n",
    "\n",
    "Could this be because the Poisson terms expect yields larger than one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [contur_model,corr_bkd,uncorr_bkd,multinorm]\n",
    "spey.set_log_level(2)\n",
    "for model in models:\n",
    "    print(model._backend.name,', asymptotic calculator, CLs: ' ,model.exclusion_confidence_level(calculator='asymptotic'))\n",
    "    print(model._backend.name,', chi_square calculator, CLs: ' ,model.exclusion_confidence_level(calculator='chi_square'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-squared calculator\n",
    "\n",
    "**Conclusion:** For the models that have a Poisson term dependence, the optimiser isn't minimising the NLL very well, resulting in a weaker CLs value\n",
    "\n",
    "From `spey.base.hypotest_base.HypothesisTestingBase`:\n",
    "\n",
    "If `poi_test_denominator=None` computes\n",
    "\n",
    "$$\\chi^2 = -2\\log\\left(\\frac{\\mathcal{L}(\\mu,\\theta_\\mu)}{\\mathcal{L}(\\hat\\mu,\\hat\\theta)}\\right)$$\n",
    "\n",
    "else\n",
    "\n",
    "$$\\chi^2 = -2\\log\\left(\\frac{\\mathcal{L}(\\mu,\\theta_\\mu)}{\\mathcal{L}(\\mu_{\\rm denom},\\theta_{\\mu_{\\rm denom}})}\\right)$$\n",
    "\n",
    "So what happens with no minimization of the NLL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at multivariate normal model first as that seems to give sensible results\n",
    "# the poi that minimises the NLL is very close to 0, so these methods give the same results\n",
    "spey.set_log_level(3)\n",
    "print(f\"without NLL minimization, CLs: {multinorm.exclusion_confidence_level(poi_test_denominator=0.0,calculator='chi_square')}\")\n",
    "print(f\"with NLL minimization, CLs: {multinorm.exclusion_confidence_level(calculator='chi_square')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try the uncorrelated background model\n",
    "# this time, the poi that minimises the NLL is close to 1.0\n",
    "spey.set_log_level(3)\n",
    "print(f\"without NLL minimization, CLs: {uncorr_bkd.exclusion_confidence_level(poi_test_denominator=0.0,calculator='chi_square')}\")\n",
    "print(f\"with NLL minimization, CLs: {uncorr_bkd.exclusion_confidence_level(calculator='chi_square')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now try the contur one\n",
    "# again the limit gets weaker when we try to do the NLL minimisation, so something is going wrong with the minimisation procedure\n",
    "# also the value it finds is very close to the uncorrelated background case, suggesting that these Poisson based likelihoods are struggling to minimise the NLL\n",
    "spey.set_log_level(3)\n",
    "print(f\"without NLL minimization, CLs: {contur_model.exclusion_confidence_level(poi_test_denominator=0.0,calculator='chi_square')}\")\n",
    "print(f\"with NLL minimization, CLs: {contur_model.exclusion_confidence_level(calculator='chi_square')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how the likelihoods with Poisson terms perform when we set the initial poi guess to zero\n",
    "\n",
    "*Conclusion:* They return a comparable exclusion to the multivariate normal model result\n",
    "\n",
    "Q. the minimisation of NLL is not dependent on which calculator is used (right?)\n",
    "\n",
    "From the log outputs, the poi is fit to a very small value ~1e-23 when `statistical_model.maximize_likelihood()` is called\n",
    "\n",
    "Q. So why in `core.fit()` is the POI fixed to 1.0? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pars=[0.,1.,1.,1.]\n",
    "corr_bkd.exclusion_confidence_level(calculator='chi_square',init_pars=init_pars,do_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contur_model.exclusion_confidence_level(calculator='chi_square',init_pars=list(np.append(0.0,np.ones(9))),do_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asymptotic calculator\n",
    "\n",
    "Q. Why is the limit 1.0 for all the models when this calculator is used?\n",
    "\n",
    "Setting the initial POI to zero, we get something more sensible\n",
    "\n",
    "Q. The initial parameter value is set to the user inputted value each time `core.fit()` is called, rather than using the `fit parameters` value returned by `statistical_model.maximize_likelihood()`, is this supposed to happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinorm.exclusion_confidence_level(calculator='asymptotic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the initial POI to zero\n",
    "multinorm.exclusion_confidence_level(calculator='asymptotic',init_pars=[0.0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spey_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
